- Main.scala
package io.github.bendyland.rtsdatapipeline

import io.github.bendyland.rtsdatapipeline.utils.SparkSessionFactory
import io.github.bendyland.rtsdatapipeline.jobs.SensorStreamJob

object Main extends App {
  val spark = SparkSessionFactory.create("KafkaConsumerApp")
  SensorStreamJob.run(spark)
}

- SensorStreamJob.scala
package io.github.bendyland.rtsdatapipeline.jobs

import org.apache.spark.sql.{SparkSession, DataFrame}
import io.github.bendyland.rtsdatapipeline.transformations.SensorDataTransformations
import io.github.bendyland.rtsdatapipeline.sinks.DataSink
import io.github.bendyland.rtsdatapipeline.messageparser.SensorData

object SensorStreamJob {
  def run(spark: SparkSession): Unit = {
    spark.sparkContext.setLogLevel("WARN")
    import spark.implicits._
    val kafkaDF = spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("subscribe", "sensor-data")
      .option("startingOffsets", "earliest")
      .load()

    val messagesDS = kafkaDF.selectExpr("CAST(value AS STRING) as message").as[String]
    val sensorDataDS = SensorDataTransformations.parseSensorData(messagesDS)(spark)
    val enrichedDF = SensorDataTransformations.enrichSensorData(sensorDataDS)(spark)

    // Get the final aggregated DataFrame from our transformation function
    val finalDF = SensorDataTransformations.preJoinAndAggregate(enrichedDF)(spark)

    // Write enriched data to Parquet
    // DataSink.writeParquet(enrichedDF, "data/parquet/enriched", "checkpoints/enriched")
    DataSink.readParquetAndWriteToConsole("data/parquet/enriched")(spark)

    // Write aggregated data to Parquet
    // DataSink.writeParquetAndAwaitTermination(finalDF, "data/parquet/aggregated", "checkpoints/aggregated")
    DataSink.readParquetAndWriteToConsole("data/parquet/aggregated")(spark)
  }
}

- MessageParser.scala
package io.github.bendyland.rtsdatapipeline.messageparser

import io.circe._, io.circe.parser._
import io.circe.generic.auto._

case class SensorData(
  Timestamp: String,
  SensorId: String,
  SensorType: String,
  Readings: Map[String, Double],
  Location: Map[String, Double],
  Status: String
)

object MessageParser {
  // Returns None if parsing failed
  def parseSensorData(jsonString: String): Option[SensorData] = {
    parse(jsonString).flatMap(_.as[SensorData]).toOption
  }
}

- DataSink.scala
package io.github.bendyland.rtsdatapipeline.sinks

import org.apache.spark.sql.{Dataset, SparkSession, DataFrame}
import org.apache.spark.sql.streaming.StreamingQuery

object DataSink {
  // Writes streaming or batch Dataset to the console
  def write(ds: Dataset[_], outputMode: String = "append"): Option[StreamingQuery] = {
    if (ds.isStreaming) {
      Some(
        ds.writeStream
          .outputMode(outputMode)
          .format("console")
          .option("truncate", false)
          .start()
      )
    } 
    else {
      ds.show(50, truncate = false)
      None
    }
  }

  // Reads a Parquet file from a given path
  def readParquet(path: String)(implicit spark: SparkSession): DataFrame = {
    spark.read.parquet(path)
  }

  // Reads a Parquet file and writes it to the console
  def readParquetAndWriteToConsole(path: String)(implicit spark: SparkSession): Unit = {
    val df = readParquet(path)
    write(df)
  }

  def writeParquet(data: DataFrame, dataPath: String, checkpointPath: String) = {
    data.writeStream
      .format("parquet")
      .option("path", dataPath)
      .option("checkpointLocation", checkpointPath)
      .outputMode("append")
      .start()
  }

  def writeParquetAndAwaitTermination(data: DataFrame, dataPath: String, checkpointPath: String) = {
    data.writeStream
      .format("parquet")
      .option("path", dataPath)
      .option("checkpointLocation", checkpointPath)
      .outputMode("append")
      .start()
      .awaitTermination()
  }

  // TODO: Add similar methods for JSON, CSV, etc.
  // def readJson(path: String)(implicit spark: SparkSession): DataFrame = ...
}

- SensorDataTransformations.scala
package io.github.bendyland.rtsdatapipeline.transformations

import org.apache.spark.sql.{Dataset, DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import io.github.bendyland.rtsdatapipeline.messageparser.{SensorData, MessageParser}

object SensorDataTransformations {
  // Parse raw JSON strings into SensorData objects.
  def parseSensorData(jsonDS: Dataset[String])(implicit spark: SparkSession): Dataset[SensorData] = {
    import spark.implicits._
    jsonDS.flatMap(MessageParser.parseSensorData)
  }

  def preJoinAndAggregate(enrichedDF: DataFrame)(implicit spark: SparkSession): DataFrame = {
    import spark.implicits._
    // Prepare the air quality stream, selecting and renaming columns explicitly:
    val aqStream = enrichedDF
      .filter($"SensorType" === "air_quality" && $"timestamp".isNotNull)
      .withWatermark("timestamp", "10 seconds")
      .withColumn("window", window($"timestamp", "2 minutes", "1 minute"))
      .withColumn("lat", round($"latitude", 4))
      .withColumn("lon", round($"longitude", 4))
      .select($"SensorType", $"window", $"lat", $"lon", $"CO")
      .alias("aq")

    // Prepare the weather stream similarly:
    val wStream = enrichedDF
      .filter($"SensorType" === "weather" && $"timestamp".isNotNull)
      .withWatermark("timestamp", "10 seconds")
      .withColumn("window", window($"timestamp", "2 minutes", "1 minute"))
      .withColumn("lat", round($"latitude", 4))
      .withColumn("lon", round($"longitude", 4))
      .select($"SensorType", $"window", $"lat", $"lon", $"temperature")
      .alias("w")

    val joinedDF = aqStream.join(wStream, Seq("window", "lat", "lon"))
    joinedDF.groupBy("window", "lat", "lon")
      .agg(
        avg($"CO").as("avg_CO"),
        avg($"temperature").as("avg_temperature")
      )
  }

  // Enrich the parsed SensorData by converting the timestamp string and extracting fields from the maps.
  def enrichSensorData(sensorDS: Dataset[SensorData])(implicit spark: SparkSession): DataFrame = {
    import spark.implicits._
    sensorDS.toDF()
      // Convert the timestamp string into a proper timestamp
      .withColumn("timestamp", to_timestamp($"Timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSSX"))
      // Extract and cast weather readings
      .withColumn("temperature", $"Readings".getItem("temperature").cast("double"))
      .withColumn("humidity", $"Readings".getItem("humidity").cast("int"))
      .withColumn("wind_speed", $"Readings".getItem("wind_speed").cast("double"))
      // Extract and cast air quality readings
      .withColumn("CO", $"Readings".getItem("CO").cast("int"))
      .withColumn("NO2", $"Readings".getItem("NO2").cast("int"))
      .withColumn("PM10", $"Readings".getItem("PM10").cast("int"))
      .withColumn("PM2_5", $"Readings".getItem("PM2_5").cast("int"))
      .withColumn("ozone", $"Readings".getItem("ozone").cast("int"))
      // Extract location coordinates 
      .withColumn("latitude", $"Location".getItem("latitude").cast("double"))
      .withColumn("longitude", $"Location".getItem("longitude").cast("double"))
  }
}

- SparkSessionFactory.scala
package io.github.bendyland.rtsdatapipeline.utils

import org.apache.spark.sql.SparkSession

object SparkSessionFactory {
  def create(appName: String): SparkSession = {
    SparkSession.builder()
      .appName(appName)
      .master("local[*]")
      .getOrCreate()
  }
}
